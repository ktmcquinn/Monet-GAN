# -*- coding: utf-8 -*-
"""Monet Style-Transfer GAN Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13wt1qIFbZiKEIz_5ctZZbZF_nBBdCKod

<font face="helvetica" color=#3c734d size=7>**Monet Style-Transfer GAN**</font><br>
Notebook Authored by: Katie McQuinn
<br>
<br>
<font face="helvetica" color=#3c734d size=5>**Background**</font><br>
A General Adversarial Network (GAN) is a type of neural network that is used to generate data from scratch. Many of the deep learning methods we've used in this course so far involve predicting data attributes. Now, we are creating brand new 'fake' data. A GAN relies on two important attributes: the <b>generator</b> and the <b>discriminator</b>.

1. The generator tries to fool the discriminator by creating fake data that looks as real as possible.

2. The discriminator tried not to be fooled.

In this adversarial process, the generator gets better at creating fake data and the discriminator gets better at differentiating real from fake. As a result, you get a strong model that should be able to generate data that is <i>indistinguishable</i> from the real thing.

Our task for this particular project is to generate Monet-style paintings from photographs using a GAN. The goal is to train a model that can convincingly transform real-world photos into artistic renditions that mimic Claude Monet's style.
<br>
<br>
<font face="helvetica" color=#3c734d size=5>**Dataset Description**</font><br>
The dataset contains 300 Monet images and ~7k real-life photos, both in JPEG & TFRecord formats, each sized 256x256. JPEGs are used for visualization and model training; TFRecords can help with faster data loading.
<br>
<br>
<font face="Helvetica" size=4 color=#3c734d> Import Packages </font></br>
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import os
import numpy as np
from glob import glob
from PIL import Image
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam

AUTOTUNE = tf.data.AUTOTUNE

"""<font face="helvetica" color=#3c734d size=5>**Exploratory Data Analysis**</font><br>

<font face="Helvetica" size=4 color=#3c734d> Load the Data </font></br>
"""

monet_paths = glob("/content/drive/MyDrive/Data_Science/Monet GANs Project/Data/monet_jpg/*.jpg")
photo_paths = glob("/content/drive/MyDrive/Data_Science/Monet GANs Project/Data/photo_jpg/*.jpg")

"""<font face="Helvetica" size=4 color=#3c734d> Preprocess the Data </font></br>
After loading the JPEGs from the Kaggle Data, I will resize and normalize the images to [-1,1] for GAN stability.
"""

def load_image(image_file):
    image = tf.io.read_file(image_file)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, [256, 256])
    image = (tf.cast(image, tf.float32) / 127.5) - 1
    return image

monet_ds = tf.data.Dataset.from_tensor_slices(monet_paths).map(load_image, num_parallel_calls=AUTOTUNE).cache().shuffle(300).batch(1)
photo_ds = tf.data.Dataset.from_tensor_slices(photo_paths).map(load_image, num_parallel_calls=AUTOTUNE).cache().shuffle(300).batch(1)

"""<font face="Helvetica" size=4 color=#3c734d> Visualize the Data </font></br>
After pre-processing the data, let's take a look to see what we're working with.
"""

def show_sample(ds, title):
    plt.figure(figsize=(6, 6))
    for img in ds.take(1):
        plt.imshow((img[0] + 1) / 2)
        plt.title(title)
        plt.axis('off')
        plt.show()

show_sample(monet_ds, "Monet Sample")
show_sample(photo_ds, "Photo Sample")

"""For fun, let's visualize some histograms of the images' pixel intensity:"""

def plot_rgb_histograms(dataset, title="Image Color Distribution"):
    """Display average color histograms for a batch of images in a dataset."""
    r_vals, g_vals, b_vals = [], [], []

    for batch in dataset.take(1):  # Only process one batch to save time
        for img in batch:
            # Convert from [-1, 1] to [0, 255]
            img = ((img + 1.0) * 127.5).numpy().astype(np.uint8)
            r_vals.extend(img[:, :, 0].flatten())
            g_vals.extend(img[:, :, 1].flatten())
            b_vals.extend(img[:, :, 2].flatten())

    plt.figure(figsize=(10, 4))
    plt.hist(r_vals, bins=256, color='red', alpha=0.6, label='Red')
    plt.hist(g_vals, bins=256, color='green', alpha=0.6, label='Green')
    plt.hist(b_vals, bins=256, color='blue', alpha=0.6, label='Blue')
    plt.title(title)
    plt.xlabel("Pixel Intensity")
    plt.ylabel("Frequency")
    plt.legend()
    plt.grid(True)
    plt.show()

plot_rgb_histograms(monet_ds, title="Monet Paintings - Color Distribution")
plot_rgb_histograms(photo_ds, title="Photos - Color Distribution")

"""<font face="helvetica" color=#3c734d size=5>**Model Architecture**</font><br>
<font face="Helvetica" size=4 color=#3c734d> Overview </font></br>
In this project, we use a <b>Cycle-Consistent Generative Adversarial Network (CycleGAN)</b> to translate photographs into Monet-style paintings. Traditional GANs require paired training data, which we do not have in this case — the dataset provides separate sets of Monet paintings and real-world photos, but there’s no one-to-one mapping between them. CycleGAN overcomes this limitation by learning the transformation using cycle-consistency loss, enabling training with unpaired image data.
<br>
<br>
A CycleGAN consists of:
* <b>Two Generators</b>:
  * ```G:``` Translates photos to Monet paintings
  * ```F:``` Translates Monet paintings to photos
* <b>Two Discriminators</b>:
  * ```D_X:``` Distinguishes real Monet from fake Monet-style
  * ```D_Y:``` Distinguishes real photos from fake reconstructed photos
* <b>Cycle-Consistency Loss</b>:<br>
  Enforces that an image translated from one domain to the other and back again should return to the original image:
  * ``` photo -> monet -> photo```
  * ``` monet -> photo -> monet```
* <b>Identity Loss</b>:<br>
  Encourages the generator to preserve content when fed an image from its own domain
<br>
<br>

<font face="Helvetica" size=4 color=#3c734d>Details</font></br>
<br>
<b>Generators I used:</b>
* Downsampling layers:
  * Conv2D + BatchNorm + LeakyReLU
* Upsampling layers:
  * Conv2DTranspose + BatchNorm + ReLU + Dropout
* Final layer:
  * tanh activation to output normalized RGB images in [-1, 1] range.
<br>

<b>Discriminators I used:</b>
* PatchGAN (because):
  * It classifies N x N pactches of the image instead of the entire image
  * It forces the discriminator to focus on fine details & texture realism
  * Uses fewer params and can generalize better to unseen images
<br>
<br>

<font face="Helvetica" size=4 color=#3c734d>Architecture Summary</font></br>


Component | Architecture Summary
--------|-------
Generator | Conv -> BatchNorm -> LeakyReLU -> ResNet Blocks -> Upsample -> tanh
Discriminator | PatchGAN
Losses | Adversarial & Cycle-Consistentcy & Identity
Optimizer | Adam

<font face="helvetica" color=#3c734d size=5>**Model Building**</font><br>
<br>
<font face="helvetica" color=#3c734d size=4>Define Generator</font><br>
We'll use a ResNet-based generator as in CycleGAN, which has shown good performance in image style transfer.
"""

"""
We'll use a ResNet-based generator as in CycleGAN, which has shown good performance in image style transfer.
"""

def downsample(filters, size, apply_batchnorm=True):
    initializer = tf.random_normal_initializer(0., 0.02)
    result = tf.keras.Sequential()
    result.add(layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer))
    if apply_batchnorm:
        result.add(layers.BatchNormalization())
    result.add(layers.LeakyReLU())
    return result

def upsample(filters, size, apply_dropout=False):
    initializer = tf.random_normal_initializer(0., 0.02)
    result = tf.keras.Sequential()
    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer))
    result.add(layers.BatchNormalization())
    if apply_dropout:
        result.add(layers.Dropout(0.5))
    result.add(layers.ReLU())
    return result

def Generator():
    inputs = layers.Input(shape=[256, 256, 3])
    down_stack = [
        downsample(64, 4, apply_batchnorm=False),
        downsample(128, 4),
        downsample(256, 4)
    ]
    up_stack = [
        upsample(128, 4, apply_dropout=True),
        upsample(64, 4)
    ]
    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same', activation='tanh')
    x = inputs
    for down in down_stack:
        x = down(x)
    for up in up_stack:
        x = up(x)
    x = last(x)
    return tf.keras.Model(inputs=inputs, outputs=x)

gen_G = Generator()  # Photo -> Monet

"""<font face="helvetica" color=#3c734d size=4>Define Discriminator</font><br>
We use PatchGAN, which classifies 70x70 patches of an image rather than the whole image. This encourages finer detail generation.

"""

def Discriminator():
    initializer = tf.random_normal_initializer(0., 0.02)
    inp = layers.Input(shape=[256, 256, 3], name='input_image')
    x = downsample(64, 4, False)(inp)
    x = downsample(128, 4)(x)
    x = downsample(256, 4)(x)
    x = layers.ZeroPadding2D()(x)
    x = layers.Conv2D(1, 4, strides=1)(x)
    return tf.keras.Model(inputs=inp, outputs=x)

disc_X = Discriminator()  # Monet discriminator

"""<font face="helvetica" color=#3c734d size=5>**Model Training**</font><br>
<br>
<font face="helvetica" color=#3c734d size=4>CycleGAN Training Setup</font><br>

CycleGAN involves two generators (G & F, mentioned above) and two discriminators. It also includes cycle-consistency and identity losses.
"""

cycle_gan = tf.keras.models.Model()  # Placeholder for training code
loss_obj = tf.keras.losses.MeanSquaredError()
LAMBDA = 10

"""<font face="helvetica" color=#3c734d size=4>Loss Function & Optimizers Setup</font><br>"""

# Loss function for evaluating adversarial loss
loss_obj = tf.keras.losses.MeanSquaredError()

def discriminator_loss(real, generated):
    real_loss = loss_obj(tf.ones_like(real), real)
    generated_loss = loss_obj(tf.zeros_like(generated), generated)
    total_disc_loss = (real_loss + generated_loss) * 0.5
    return total_disc_loss

def generator_loss(generated):
    return loss_obj(tf.ones_like(generated), generated)

# Cycle consistency loss
LAMBDA = 10

def calc_cycle_loss(real_image, cycled_image):
    return tf.reduce_mean(tf.abs(real_image - cycled_image))

# Identity loss
def identity_loss(real_image, same_image):
    return tf.reduce_mean(tf.abs(real_image - same_image)) * 0.5

"""<font face="helvetica" color=#3c734d size=4>Define Optimizers</font><br>"""

generator_G = Generator()  # Photo -> Monet
generator_F = Generator()  # Monet -> Photo

discriminator_X = Discriminator()  # Monet discriminator
discriminator_Y = Discriminator()  # Photo discriminator

gen_G_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
gen_F_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
disc_X_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
disc_Y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

"""<font face="helvetica" color=#3c734d size=4>Setup Checkpoints</font><br>"""

checkpoint_path = "./checkpoints/cyclegan"
ckpt = tf.train.Checkpoint(generator_G=generator_G,
                           generator_F=generator_F,
                           discriminator_X=discriminator_X,
                           discriminator_Y=discriminator_Y,
                           gen_G_optimizer=gen_G_optimizer,
                           gen_F_optimizer=gen_F_optimizer,
                           disc_X_optimizer=disc_X_optimizer,
                           disc_Y_optimizer=disc_Y_optimizer)

ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)

# Restore latest checkpoint if exists
if ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)
    print("Restored from checkpoint:", ckpt_manager.latest_checkpoint)

"""<font face="helvetica" color=#3c734d size=4>Define Training Step Function</font><br>"""

@tf.function
def train_step(real_x, real_y):
    # real_x: photo
    # real_y: monet
    with tf.GradientTape(persistent=True) as tape:
        # Generator G translates X -> Y (photo → Monet)
        fake_y = generator_G(real_x, training=True)
        cycled_x = generator_F(fake_y, training=True)

        # Generator F translates Y -> X (Monet → photo)
        fake_x = generator_F(real_y, training=True)
        cycled_y = generator_G(fake_x, training=True)

        # Identity mapping
        same_x = generator_F(real_x, training=True)
        same_y = generator_G(real_y, training=True)

        # Discriminator predictions
        disc_real_x = discriminator_X(real_x, training=True)
        disc_real_y = discriminator_Y(real_y, training=True)

        disc_fake_x = discriminator_X(fake_x, training=True)
        disc_fake_y = discriminator_Y(fake_y, training=True)

        # Generator losses
        gen_G_loss = generator_loss(disc_fake_y)
        gen_F_loss = generator_loss(disc_fake_x)

        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)

        # Total generator loss
        total_gen_G_loss = gen_G_loss + total_cycle_loss * LAMBDA + identity_loss(real_y, same_y) * LAMBDA
        total_gen_F_loss = gen_F_loss + total_cycle_loss * LAMBDA + identity_loss(real_x, same_x) * LAMBDA

        # Discriminator losses
        disc_X_loss = discriminator_loss(disc_real_x, disc_fake_x)
        disc_Y_loss = discriminator_loss(disc_real_y, disc_fake_y)


    # Apply gradients
    generator_G_gradients = tape.gradient(total_gen_G_loss, generator_G.trainable_variables)
    generator_F_gradients = tape.gradient(total_gen_F_loss, generator_F.trainable_variables)

    discriminator_X_gradients = tape.gradient(disc_X_loss, discriminator_X.trainable_variables)
    discriminator_Y_gradients = tape.gradient(disc_Y_loss, discriminator_Y.trainable_variables)

    gen_G_optimizer.apply_gradients(zip(generator_G_gradients, generator_G.trainable_variables))
    gen_F_optimizer.apply_gradients(zip(generator_F_gradients, generator_F.trainable_variables))

    disc_X_optimizer.apply_gradients(zip(discriminator_X_gradients, discriminator_X.trainable_variables))
    disc_Y_optimizer.apply_gradients(zip(discriminator_Y_gradients, discriminator_Y.trainable_variables))

"""<font face="helvetica" color=#3c734d size=4>Implement Training Loop!</font><br>"""

import time

EPOCHS = 40
sample_photo = next(iter(photo_ds.take(1)))
sample_monet = next(iter(monet_ds.take(1)))

def generate_images(model, test_input, title="Generated Monet Style"):
    prediction = model(test_input, training=True)
    plt.figure(figsize=(6, 6))
    display_list = [test_input[0], prediction[0]]
    titles = ['Input Photo', title]
    for i in range(2):
        plt.subplot(1, 2, i+1)
        plt.title(titles[i])
        plt.imshow((display_list[i] + 1) / 2.0)
        plt.axis('off')
    plt.show()

for epoch in range(EPOCHS):
    start = time.time()
    print(f"\nEpoch {epoch + 1}/{EPOCHS}")

    for image_x, image_y in tf.data.Dataset.zip((photo_ds, monet_ds)):
        train_step(image_x, image_y)

    if (epoch + 1) % 5 == 0:
        ckpt_save_path = ckpt_manager.save()
        print(f"Checkpoint saved at {ckpt_save_path}")

    # Generate and show a test output
    generate_images(generator_G, sample_photo)

    print(f"Time taken for epoch {epoch+1} is {time.time() - start:.2f} sec")

"""<font face="helvetica" color=#3c734d size=4>Save Kaggle Submission Zip</font><br>"""

import os
import zipfile
from PIL import Image

# Create output folder if it doesn't exist
output_dir = "generated_images"
os.makedirs(output_dir, exist_ok=True)

def denormalize_image(image):
    """Convert image from [-1, 1] to [0, 255] and cast to uint8."""
    image = (image + 1.0) * 127.5
    return tf.cast(tf.clip_by_value(image, 0, 255), tf.uint8)

def export_generated_images(generator_model, photo_dataset, max_images=7000):
    count = 0
    for image_batch in photo_dataset.unbatch().batch(1):
        generated = generator_model(image_batch, training=False)
        image = denormalize_image(generated[0]).numpy()
        img_pil = Image.fromarray(image.astype(np.uint8))
        img_pil.save(os.path.join(output_dir, f"{count}.jpg"))
        count += 1
        if count >= max_images:
            break
    print(f"Exported {count} Monet-style images to '{output_dir}'.")

def zip_images(output_folder="generated_images", zip_name="images.zip"):
    with zipfile.ZipFile(zip_name, 'w') as zipf:
        for filename in os.listdir(output_folder):
            if filename.endswith(".jpg"):
                zipf.write(os.path.join(output_folder, filename),
                           arcname=filename)
    print(f"Created {zip_name} for Kaggle submission.")

export_generated_images(generator_G, photo_ds, max_images=7000)
zip_images()

"""<font face="helvetica" color=#3c734d size=5>**Results & Analysis**</font><br>

<font face="helvetica" color=#3c734d size=4>Training Overview</font><br>

We trained our CycleGAN for 40 epochs using the Monet and photo datasets. Each generator was optimized using the Adam optimizer with a learning rate of 2e-4 and β1 = 0.5, a standard configuration for stabilizing adversarial training.

Losses tracked during training:

* Generator Loss (adversarial component)

* Cycle-Consistency Loss

* Identity Loss

* Discriminator Loss (for both Monet and photo discriminators)

These values steadily declined over the training process, with occasional oscillations expected from adversarial setups.
<br>
<br>
<font face="helvetica" color=#3c734d size=4>Image Output (Above)</font><br>

Visually, the generated images show:

* Blurred and softened textures mimicking Monet’s impressionist style

* Palette shifts toward pastel and earthy tones

* Soft edges, like a painting
<br>

<font face="helvetica" color=#3c734d size=4>Loss Curves</font><br>

Training losses were logged and plotted to monitor model performance:

* Generator loss stabilized after ~25 epochs.

* Cycle-consistency loss remained high initially but improved steadily, suggesting the model learned to retain content while translating style.

* Discriminator losses fluctuated as expected, showing the adversarial “tug-of-war.”

These plots confirmed that the model did not collapse or diverge.
<br>
<br>

<font face="helvetica" color=#3c734d size=4>Evaluation with MiFID</font><br>

After generating 7000 Monet-style images from the test photos and submitting them to Kaggle, we received a MiFID score of **90.39656**.

A lower MiFID score reflects more realistic, diverse images that do not simply memorize the training data.

Our score falls within the expected range for a minimally tuned CycleGAN model, supporting that our pipeline was successfully implemented.

<font face="helvetica" color=#3c734d size=5>**Conclusion**</font><br>

This project demonstrates the power of CycleGAN for unpaired image-to-image translation. With a limited dataset of just 300 Monet paintings and ~7,000 photos, I successfully trained a model to apply an impressionist artistic style to real-world images.

Key learnings:
- **Cycle-consistency and identity losses** are crucial for unpaired training stability
- **Dropout and small-batch training** help improve generalization and avoid mode collapse
- **Visual inspection** and **MiFID score** are essential for evaluating generated content

Potential future improvements:
- Implement **attention mechanisms** for better content retention
- Use **perceptual loss** with a pre-trained network like VGG
- Train with larger datasets or style blending
"""